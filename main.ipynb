{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a2443d",
   "metadata": {},
   "source": [
    "# Przewidywanie bankructwa polskich przedsiębiorstw\n",
    "\n",
    "Celem projektu jest zbudowanie modelu klasyfikacyjnego, który na podstawie wskaźników finansowych przewidzi upadłość firmy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a1e56",
   "metadata": {},
   "source": [
    "### Importowanie bibliotek i konfiguracja środowiska\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca428fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from scipy.io import arff\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    recall_score,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from attributes import attributes_pl\n",
    "\n",
    "# Ustawienia wyświetlania\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "# Ustawienia kolorów\n",
    "COLOR_HEALTHY = \"#1f77b4\"\n",
    "COLOR_BANKRUPT = \"#d62728\"\n",
    "\n",
    "YEAR_TO_ANALYZE = 3  # od 1 do 5\n",
    "print(f\"Analizowany rok: {YEAR_TO_ANALYZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd93e0",
   "metadata": {},
   "source": [
    "## Część 1: Inżynieria danych i model bazowy\n",
    "\n",
    "1.  **Wczytanie i unifikacja danych:** Wczytanie plików `.arff` i unifikacja.\n",
    "2.  **Eksploracyjna analiza danych (EDA):** Zrozumienie danych, braków i korelacji.\n",
    "3.  **Przetworzenie i podział danych:** Czyszczenie, podział, imputacja i skalowanie danych.\n",
    "4.  **Analiza głównych składowych (PCA):** Redukcja wymiarowości.\n",
    "5.  **Model bazowy (baseline):** Budowa prostego modelu odniesienia (regresja logistyczna).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec44ca5",
   "metadata": {},
   "source": [
    "### Wczytanie i unifikacja danych\n",
    "\n",
    "Wczytanie danych z pliku `.arff`. Dane zawierają wskaźniki finansowe (Attr1 - Attr64) oraz etykietę klasy (`class`), gdzie:\n",
    "\n",
    "- `0` - firma zdrowa\n",
    "- `1` - bankrut\n",
    "\n",
    "Następnie dane są dzielone zbiór na treningowy i testowy (proporcja 80/20).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"data/{YEAR_TO_ANALYZE}year.arff\"\n",
    "\n",
    "try:\n",
    "    data, meta = arff.loadarff(file_path)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df[\"class\"] = df[\"class\"].astype(int)\n",
    "\n",
    "    print(f\"Wczytano dane dla roku {YEAR_TO_ANALYZE}\")\n",
    "    print(f\"Wymiary: {df.shape[0]} wierszy, {df.shape[1]} kolumn\")\n",
    "\n",
    "    display(df.head(5))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"BŁĄD: Nie odnaleziono pliku: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed77d7",
   "metadata": {},
   "source": [
    "### Eksploracyjna analiza danych (EDA)\n",
    "\n",
    "1.  **Analiza braków danych:** Sprawdzenie, które wskaźniki finansowe są najczęściej niekompletne.\n",
    "2.  **Rozkład klas:** Weryfikacja, jak bardzo niezbalansowany jest zbiór (stosunek firm zdrowych do bankrutów).\n",
    "3.  **Korelacje:** Szukanie cech, które mają najsilniejszy związek (dodatni lub ujemny) z bankructwem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b24f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustawienia kolorów\n",
    "BINARY_PALETTE = [COLOR_HEALTHY, COLOR_BANKRUPT]\n",
    "\n",
    "cmap_diverging = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"CustomRdBu\", [COLOR_HEALTHY, \"white\", COLOR_BANKRUPT]\n",
    ")\n",
    "\n",
    "# 1. ANALIZA BRAKÓW DANYCH\n",
    "missing = df.isnull().sum() / len(df) * 100\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if not missing.empty:\n",
    "    top_missing = missing.head(20)\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=top_missing.index,\n",
    "        y=top_missing.values,\n",
    "        hue=top_missing.index,\n",
    "        legend=False,\n",
    "        palette=sns.light_palette(COLOR_BANKRUPT, n_colors=25, reverse=True),\n",
    "    )\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.1f%%\", padding=3)\n",
    "\n",
    "    plt.title(\"Cechy z największymi brakami danych\")\n",
    "    plt.xlabel(\"Cecha\")\n",
    "    plt.ylabel(\"% braków\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    max_missing = top_missing.max()\n",
    "    limit = max_missing * 1.15 if max_missing > 0 else 10\n",
    "\n",
    "    plt.ylim(0, limit)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(\"LEGENDA\")\n",
    "    for attr in top_missing.index[:20]:\n",
    "        print(f\"{attr}: {attributes_pl.get(attr, 'Brak opisu')}\")\n",
    "else:\n",
    "    print(\"Brak pustych wartości\")\n",
    "\n",
    "# 2. ROZKŁAD KLAS\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=\"class\", data=df, hue=\"class\", legend=False, palette=BINARY_PALETTE\n",
    ")\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "for container in ax.containers:\n",
    "    labels = [\n",
    "        f\"{int(v.get_height())} ({v.get_height() / total * 100:.2f}%)\"\n",
    "        for v in container\n",
    "    ]\n",
    "    ax.bar_label(container, labels=labels, label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.title(\"Liczba firm według statusu\")\n",
    "plt.xlabel(\"Status firmy\")\n",
    "plt.ylabel(\"Liczba firm\")\n",
    "plt.xticks([0, 1], [\"0 (Zdrowa)\", \"1 (Bankrut)\"])\n",
    "\n",
    "max_height = max([p.get_height() for p in ax.patches])\n",
    "\n",
    "plt.ylim(0, max_height * 1.2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 3. KORELACJE\n",
    "correlations = df.corr()[\"class\"].sort_values()\n",
    "top_corr = correlations.abs().sort_values(ascending=False).head(11)\n",
    "top_corr_names = top_corr.index.tolist()\n",
    "\n",
    "if \"class\" in top_corr_names:\n",
    "    top_corr_names.remove(\"class\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "corr_values = correlations[top_corr_names].values\n",
    "norm = plt.Normalize(corr_values.min(), corr_values.max())\n",
    "colors = cmap_diverging(norm(corr_values))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=corr_values,\n",
    "    y=top_corr_names,\n",
    "    hue=corr_values,\n",
    "    legend=False,\n",
    "    palette=cmap_diverging,\n",
    ")\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.2f\", padding=3)\n",
    "\n",
    "plt.title(\"Cechy najsilniej skorelowane z bankructwem\")\n",
    "plt.xlabel(\"Współczynnik korelacji (Pearson)\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "min_val = corr_values.min()\n",
    "max_val = corr_values.max()\n",
    "padding = max(abs(min_val), abs(max_val)) * 0.2\n",
    "\n",
    "plt.xlim(min_val - padding, max_val + padding)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"LEGENDA\")\n",
    "for attr in top_corr_names:\n",
    "    print(f\"{attr}: {attributes_pl.get(attr, 'Brak opisu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd1b9b",
   "metadata": {},
   "source": [
    "### Przetworzenie i podział danych\n",
    "\n",
    "Na podstawie analizy braków danych zdecydowano usunąć cechę `Attr37`, która posiada zbyt wiele pustych wartości, by je bezpiecznie uzupełniać.\n",
    "\n",
    "Następnie:\n",
    "\n",
    "1.  **Podział (Train/Test):** Dane dzielone są w proporcji 80/20 z zachowaniem proporcji klas (`stratify`).\n",
    "2.  **Pipeline:**\n",
    "    - **Imputacja:** Braki uzupełniane są medianą.\n",
    "    - **Skalowanie:** Dane są standaryzowane (`StandardScaler`), co jest wymagane dla PCA i regresji logistycznej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PRZYGOTOWANIE DANYCH\n",
    "# Usunięcie cechy Attr37\n",
    "if \"Attr37\" in df.columns:\n",
    "    X = df.drop([\"class\", \"Attr37\"], axis=1)\n",
    "    print(\"Usunięto kolumnę Attr37\")\n",
    "else:\n",
    "    X = df.drop(\"class\", axis=1)\n",
    "    print(\"Kolumna Attr37 nie istnieje\")\n",
    "\n",
    "y = df[\"class\"]\n",
    "\n",
    "feature_names_final = X.columns.tolist()\n",
    "print(f\"Liczba cech: {X.shape[1]}\")\n",
    "\n",
    "# 2. PODZIAŁ DANYCH\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {X_train.shape}\")\n",
    "print(f\"Rozmiar zbioru testowego:  {X_test.shape}\")\n",
    "print(f\"Liczba bankrutów w zbiorze testowym: {y_test.sum()} (na {len(y_test)} firm)\")\n",
    "\n",
    "# 3. PIPELINE PRZETWARZANIA DANYCH\n",
    "preprocessor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputer\",\n",
    "            SimpleImputer(strategy=\"median\"),\n",
    "        ),  # mediana jest odporna na wartości odstające\n",
    "        (\"scaler\", StandardScaler()),  # średnia=0, odchylenie=1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. DOPASOWANIE I TRANSFORMACJA DANYCH\n",
    "# Pipeline jest dopasowywany do zbioru treningowego\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "# Zbiór testowy jest transformowany na podstawie parametrów wyuczonych na zbiorze treningowym\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "# Zamiana na DataFrame (dla wygody operowania nazwami kolumn)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names_final)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad0320",
   "metadata": {},
   "source": [
    "### Analiza głównych składowych (PCA)\n",
    "\n",
    "Dane mają 63 wymiary, co utrudnia wizualizację. Zastosowano PCA, aby sprawdzić, czy bankruci tworzą oddzielne skupisko.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustawienia kolorów\n",
    "palette_dict = {\"Zdrowa\": COLOR_HEALTHY, \"Bankrut\": COLOR_BANKRUPT}\n",
    "\n",
    "# 1. URUCHOMIENIE PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca[\"class\"] = y_train.values\n",
    "df_pca[\"Legenda\"] = df_pca[\"class\"].map({0: \"Zdrowa\", 1: \"Bankrut\"})\n",
    "\n",
    "# 2. LICZBA WARIANCJI WYJAŚNIONEJ\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(f\"Wariancja wyjaśniona przez PC1: {evr[0]:.2%}\")\n",
    "print(f\"Wariancja wyjaśniona przez PC2: {evr[1]:.2%}\")\n",
    "print(f\"Suma informacji na wykresie: {sum(evr):.2%}\")\n",
    "\n",
    "# 3. WIZUALIZACJA PCA\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"Legenda\",\n",
    "    data=df_pca,\n",
    "    palette=palette_dict,\n",
    "    alpha=0.8,\n",
    "    edgecolor=None,\n",
    ")\n",
    "plt.title(\"PCA: Przestrzeń 2D cech\")\n",
    "plt.xlabel(f\"PC1 ({evr[0]:.2%} wariancji)\")\n",
    "plt.ylabel(f\"PC2 ({evr[1]:.2%} wariancji)\")\n",
    "plt.legend(title=\"Status firmy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 4. WYBÓR LICZBY KOMPONENTÓW PCA\n",
    "pca_full = PCA().fit(X_train_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(\n",
    "    np.cumsum(pca_full.explained_variance_ratio_),\n",
    "    marker=\".\",\n",
    "    linestyle=\"-\",\n",
    "    color=COLOR_HEALTHY,\n",
    ")\n",
    "plt.xlabel(\"Liczba komponentów\")\n",
    "plt.ylabel(\"Skumulowana wariancja wyjaśniona\")\n",
    "plt.title(\"Liczba komponentów PCA a wyjaśniona wariancja\")\n",
    "plt.axhline(y=0.95, color=COLOR_BANKRUPT, linestyle=\"--\", label=\"Próg 95%\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc21690",
   "metadata": {},
   "source": [
    "### Model bazowy (baseline): Regresja logistyczna\n",
    "\n",
    "Jako punkt odniesienia wytrenowana została regresja logistyczna.\n",
    "Używany jest parametr `class_weight='balanced'`, aby zmusić model do zwracania uwagi na mniejszą klasę bankrutów (w przeciwnym razie model mógłby ignorować bankrutów i wciąż mieć wysoką ogólną dokładność).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRENING\n",
    "log_reg = LogisticRegression(class_weight=\"balanced\", max_iter=2000, random_state=42)\n",
    "\n",
    "print(\"Trening modelu bazowego...\")\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. PREDYKCJA\n",
    "y_pred_base = log_reg.predict(X_test_scaled)\n",
    "y_proba_base = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 3. WYNIKI\n",
    "roc_auc_base = roc_auc_score(y_test, y_proba_base)\n",
    "recall_base = recall_score(y_test, y_pred_base)\n",
    "f1_score_base = f1_score(y_test, y_pred_base)\n",
    "\n",
    "print(\"WYNIKI\")\n",
    "print(f\"ROC AUC: {roc_auc_base:.4f}\")\n",
    "print(f\"Recall: {recall_base:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_base:.4f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "\n",
    "# 4. WIZUALIZACJE (macierz pomyłek i krzywa ROC)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Macierz Pomyłek\n",
    "cmap_cm = sns.light_palette(COLOR_HEALTHY, as_cmap=True)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_base)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap_cm, cbar=False, ax=ax[0])\n",
    "\n",
    "ax[0].set_title(\"Macierz pomyłek\")\n",
    "ax[0].set_xlabel(\"Przewidziana klasa\")\n",
    "ax[0].set_ylabel(\"Prawdziwa klasa\")\n",
    "ax[0].set_xticklabels([\"Zdrowa\", \"Bankrut\"])\n",
    "ax[0].set_yticklabels([\"Zdrowa\", \"Bankrut\"])\n",
    "\n",
    "# Krzywa ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_base)\n",
    "\n",
    "ax[1].plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    label=f\"Regresja logistyczna (ROC AUC = {roc_auc_base:.2f})\",\n",
    "    color=COLOR_HEALTHY,\n",
    "    linewidth=3,\n",
    ")\n",
    "ax[1].fill_between(fpr, tpr, color=COLOR_HEALTHY, alpha=0.1)\n",
    "ax[1].plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "ax[1].set_xlabel(\"False Positive (fałszywe alarmy)\")\n",
    "ax[1].set_ylabel(\"True Positive (wykrywalność)\")\n",
    "ax[1].set_title(\"Krzywa ROC\")\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97679773",
   "metadata": {},
   "source": [
    "## Część 2: Zaawansowane modelowanie\n",
    "\n",
    "1.  **Random Forest:** Model odporny na overfitting.\n",
    "2.  **XGBoost:** Obecny standard w konkursach ML.\n",
    "3.  **Threshold tuning:** Manipulacja progiem decyzyjnym, aby zmaksymalizować wykrywalność bankrutów (recall).\n",
    "4.  **WandB:** Śledzenie eksperymentów w chmurze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca131ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import słownika\n",
    "try:\n",
    "    from attributes import attributes_pl\n",
    "except ImportError:\n",
    "    attributes_pl = {}\n",
    "\n",
    "# 1. ZAŁADOWANIE KONFIGURACJI\n",
    "load_dotenv()\n",
    "\n",
    "# Pobranie klucza API\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"UWAGA: Nie znaleziono WANDB_API_KEY\")\n",
    "\n",
    "try:\n",
    "    print(\"Logowanie do WandB...\")\n",
    "    wandb.login(key=api_key)\n",
    "    print(\"Zalogowano pomyślnie\")\n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD: Nieudana próba logowania do WandB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a01c47",
   "metadata": {},
   "source": [
    "### Eksperyment 1: Las losowe (random forest)\n",
    "\n",
    "Algorytm, który buduje wiele drzew decyzyjnych i uśrednia ich wyniki.\n",
    "\n",
    "- **Strategia na niezbalansowanie:** Użyto `class_weight='balanced'`, co automatycznie zwiększa kary za błędy na klasie mniejszościowej (bankrutach).\n",
    "- **Feature importance:** Model ten pozwala łatwo ocenić, które wskaźniki finansowe są kluczowe dla predykcji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINICJA WARIANTÓW PARAMETRÓW\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500, 1000],  # liczba drzew\n",
    "    \"max_depth\": [10, 20, None],  # głębokość drzew\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # minimalna liczba próbek w liściu\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    \"model_type\": \"Random Forest Tuned\",\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 1,\n",
    "    \"year\": YEAR_TO_ANALYZE,\n",
    "}\n",
    "\n",
    "# 2. SZUKANIE NAJLEPSZEGO MODELU\n",
    "CV_FOLDS = 3  # liczba podziałów walidacji krzyżowej\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "total_trainings = total_combinations * CV_FOLDS\n",
    "print(\n",
    "    f\"Szukanie najlepszego modelu ({total_combinations} kombinacji, {total_trainings} treningów)...\"\n",
    ")\n",
    "start_search = time.time()\n",
    "\n",
    "rf_temp = RandomForestClassifier(class_weight=\"balanced\", random_state=42, n_jobs=1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_temp,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # szukanie najwyższego ROC AUC\n",
    "    cv=CV_FOLDS,  # walidacja krzyżowa\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Poszukiwania zakończono w {time.time() - start_search:.2f} s\")\n",
    "\n",
    "# 3. WYBÓR ZWYCIĘZCY\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"NAJLEPSZE HIPERPARAMETRY:\")\n",
    "print(f\"Liczba drzew (n_estimators): {best_params['n_estimators']}\")\n",
    "print(f\"Głębokość drzew (max_depth): {best_params['max_depth']}\")\n",
    "print(\n",
    "    f\"minimalna liczba próbek w liściu (min_samples_leaf): {best_params['min_samples_leaf']}\"\n",
    ")\n",
    "\n",
    "# 4. KONFIGURACJA RUNU\n",
    "run_config_best = fixed_params.copy()\n",
    "run_config_best.update(best_params)\n",
    "\n",
    "# 5. INICJALIZACJA RUNU\n",
    "run = wandb.init(\n",
    "    project=\"polish-bankruptcy-prediction\",\n",
    "    config=run_config_best,\n",
    "    name=f\"RF_TUNED_Year{YEAR_TO_ANALYZE}\",\n",
    ")\n",
    "\n",
    "# 6. EWALUACJA ZWYCIĘZCY\n",
    "rf_model = best_rf_model\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_score_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"WYNIKI\")\n",
    "print(f\"ROC AUC: {roc_auc_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_rf:.4f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# 7. WIZUALIZACJA WAŻNOŚCI CECH\n",
    "importances = rf_model.feature_importances_\n",
    "feature_imp_df = (\n",
    "    pd.DataFrame({\"Feature\": X_train_scaled.columns, \"Importance\": importances})\n",
    "    .sort_values(by=\"Importance\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "feature_imp_df[\"Opis\"] = (\n",
    "    feature_imp_df[\"Feature\"].map(attributes_pl).fillna(feature_imp_df[\"Feature\"])\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = sns.light_palette(COLOR_HEALTHY, n_colors=10, reverse=True)\n",
    "ax = sns.barplot(\n",
    "    x=\"Importance\",\n",
    "    y=\"Opis\",\n",
    "    hue=\"Opis\",\n",
    "    data=feature_imp_df,\n",
    "    palette=custom_palette,\n",
    "    legend=False,\n",
    ")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.4f\", padding=3)\n",
    "\n",
    "plt.title(\"Najważniejsze cechy\")\n",
    "plt.xlabel(\"Ważność (wskaźnik Giniego)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlim(0, feature_imp_df[\"Importance\"].max() * 1.15)\n",
    "plt.show()\n",
    "\n",
    "# 8. LOGOWANIE WYNIKÓW DO WANDB\n",
    "wandb.log(\n",
    "    {\n",
    "        \"roc_auc\": roc_auc_rf,\n",
    "        \"recall\": recall_rf,\n",
    "        \"f1_score\": f1_score_rf,\n",
    "        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=y_test.values,\n",
    "            preds=y_pred_rf,\n",
    "            class_names=[\"Zdrowa\", \"Bankrut\"],\n",
    "        ),\n",
    "        \"roc_curve\": wandb.plot.roc_curve(\n",
    "            y_test.values,\n",
    "            rf_model.predict_proba(X_test_scaled),\n",
    "            labels=[\"Zdrowa\", \"Bankrut\"],\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8e502",
   "metadata": {},
   "source": [
    "### Analiza progu decyzyjnego (threshold tuning)\n",
    "\n",
    "Większość modeli domyślnie klasyfikuje firmę jako bankruta, jeśli prawdopodobieństwo wynosi $> 50\\%$. W przypadku danych niezbalansowanych to podejście często zawodzi – model jest zbyt _ostrożny_.\n",
    "\n",
    "Poniżej analizowana jest **krzywa precyzja-czułość**, która przedstawia dylemat:\n",
    "\n",
    "- Czy wykrywać wszystkich bankrutów (**wysoka czułość**), ale mieć dużo fałszywych alarmów?\n",
    "- Czy mieć pewność, że oznaczone przedsiębiorstwo na pewno zbankrutuje (**wysoka precyzja**), ale wielu zostanie przeoczonych?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINICJA NOWEGO PROGU\n",
    "NEW_THRESHOLD = 0.2\n",
    "print(f\"Nowy próg decyzyjny: {NEW_THRESHOLD}\")\n",
    "\n",
    "# 2. DOSTOSOWANIE PREDYKCJI\n",
    "y_pred_adjusted = (y_proba_rf >= NEW_THRESHOLD).astype(int)\n",
    "\n",
    "# 3. SPRAWDZENIE NOWYCH WYNIKÓW\n",
    "print(\"WYNIKI\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "\n",
    "# 4. WIZUALIZACJA DYLEMATU\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    thresholds,\n",
    "    precisions[:-1],\n",
    "    label=\"Precyzja\",\n",
    "    color=COLOR_HEALTHY,\n",
    "    linewidth=2,\n",
    "    linestyle=\"--\"\n",
    ")\n",
    "plt.plot(thresholds, recalls[:-1], label=\"Czułość\", color=COLOR_BANKRUPT, linewidth=2)\n",
    "plt.xlabel(\"Próg decyzyjny (threshold)\")\n",
    "plt.ylabel(\"Wartość metryki\")\n",
    "plt.title(\"Dylemat: precyzja a czułość\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=NEW_THRESHOLD, color=\"gray\", linestyle=\":\", label=f\"Próg {NEW_THRESHOLD}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e346ea",
   "metadata": {},
   "source": [
    "### Eksperyment 2: XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Aby poradzić sobie z małą liczbą bankrutów, zastosowano parametr `scale_pos_weight`. Mówi on modelowi, że **błąd na bankrucie jest X razy bardziej kosztowny** niż błąd na zdrowej firmie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. OBLICZENIE WAGI KLASY MNIEJSZOŚCIOWEJ\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Wyliczona waga dla klasy bankrutów: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 2. DEFINICJA WARIANTÓW PARAMETRÓW\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500, 1000],  # liczba drzew\n",
    "    \"max_depth\": [4, 6, 8, 10],  # głębokość drzew\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # szybkość uczenia\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    \"model_type\": \"XGBoost Tuned\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"year\": YEAR_TO_ANALYZE,\n",
    "}\n",
    "\n",
    "# 3. SZUKANIE NAJLEPSZEGO MODELU\n",
    "CV_FOLDS = 3 # liczba podziałów walidacji krzyżowej\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "total_trainings = total_combinations * CV_FOLDS\n",
    "print(\n",
    "    f\"Szukanie najlepszego modelu ({total_combinations} kombinacji, {total_trainings} treningów)...\"\n",
    ")\n",
    "start_search = time.time()\n",
    "\n",
    "xgb_temp = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    eval_metric=\"auc\",\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_temp,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",  # szukanie najwyższego ROC AUC\n",
    "    cv=CV_FOLDS,  # walidacja krzyżowa\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Poszukiwania zakończono w {time.time() - start_search:.2f} s\")\n",
    "\n",
    "# 4. WYBÓR ZWYCIĘZCY\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"NAJLEPSZE HIPERPARAMETRY:\")\n",
    "print(f\"Liczba drzew (n_estimators): {best_params['n_estimators']}\")\n",
    "print(f\"Głębokość drzew (max_depth): {best_params['max_depth']}\")\n",
    "print(f\"Szybkość uczenia (learning_rate): {best_params['learning_rate']}\")\n",
    "\n",
    "# 5. KONFIGURACJA RUNU\n",
    "run_config_best = fixed_params.copy()\n",
    "run_config_best.update(best_params)\n",
    "\n",
    "# 6. INICJALIZACJA RUNU\n",
    "run = wandb.init(\n",
    "    project=\"polish-bankruptcy-prediction\",\n",
    "    config=run_config_best,\n",
    "    name=f\"XGB_TUNED_Year{YEAR_TO_ANALYZE}\",\n",
    ")\n",
    "\n",
    "# 7. EWALUACJA\n",
    "xgb_model = best_xgb_model\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_score_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"WYNIKI\")\n",
    "print(f\"ROC AUC: {roc_auc_xgb:.4f}\")\n",
    "print(f\"Recall: {recall_xgb:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_xgb:.4f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# 8. WIZUALIZACJA WAŻNOŚCI CECH\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_imp_df = (\n",
    "    pd.DataFrame({\"Feature\": X_train_scaled.columns, \"Importance\": importances})\n",
    "    .sort_values(by=\"Importance\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "feature_imp_df[\"Opis\"] = (\n",
    "    feature_imp_df[\"Feature\"].map(attributes_pl).fillna(feature_imp_df[\"Feature\"])\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "custom_palette = sns.light_palette(COLOR_HEALTHY, n_colors=10, reverse=True)\n",
    "ax = sns.barplot(\n",
    "    x=\"Importance\",\n",
    "    y=\"Opis\",\n",
    "    hue=\"Opis\",\n",
    "    data=feature_imp_df,\n",
    "    palette=custom_palette,\n",
    "    legend=False,\n",
    ")\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.4f\", padding=3)\n",
    "\n",
    "plt.title(\"Najważniejsze cechy\")\n",
    "plt.xlabel(\"Ważność (gain)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlim(0, feature_imp_df[\"Importance\"].max() * 1.15)\n",
    "plt.show()\n",
    "\n",
    "# 9. LOGOWANIE WYNIKÓW DO WANDB\n",
    "wandb.log(\n",
    "    {\n",
    "        \"roc_auc\": roc_auc_xgb,\n",
    "        \"recall\": recall_xgb,\n",
    "        \"f1_score\": f1_score_xgb,\n",
    "        \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=y_test.values,\n",
    "            preds=y_pred_xgb,\n",
    "            class_names=[\"Zdrowa\", \"Bankrut\"],\n",
    "        ),\n",
    "        \"roc_curve\": wandb.plot.roc_curve(\n",
    "            y_test.values,\n",
    "            xgb_model.predict_proba(X_test_scaled),\n",
    "            labels=[\"Zdrowa\", \"Bankrut\"],\n",
    "        ),\n",
    "        \"feature_importance\": wandb.plot.bar(\n",
    "            wandb.Table(dataframe=feature_imp_df[[\"Feature\", \"Importance\"]]),\n",
    "            \"Feature\",\n",
    "            \"Importance\",\n",
    "            title=\"Najważniejsze cechy\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa96778",
   "metadata": {},
   "source": [
    "### Finalna optymalizacja: Dostrojenie progu pod biznes\n",
    "\n",
    "W kontekście bankowym/inwestycyjnym koszt przeoczenia bankructwa (utrata kapitału) jest drastycznie wyższy niż koszt sprawdzenia fałszywego alarmu.\n",
    "\n",
    "Dlatego w tym kroku **nie ufa się domyślnemu progowi**. Ustawiono cel biznesowy na **wykrycie co najmniej 85% bankrutów** (recall $\\approx$ 0,85) i sprawdzono, jaki próg prawdopodobieństwa pozwoli to osiągnąć.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PRZYGOTOWANIE KONFIGURACJI\n",
    "opt_config = run_config_best.copy()\n",
    "opt_config[\"threshold_tuning\"] = True\n",
    "opt_config[\"model_stage\"] = \"optimization\"\n",
    "\n",
    "# 2. INICJALIZACJA RUNU\n",
    "run = wandb.init(\n",
    "    project=\"polish-bankruptcy-prediction\",\n",
    "    name=f\"XGB_Optimization_Year{YEAR_TO_ANALYZE}\",\n",
    "    config=opt_config,\n",
    ")\n",
    "\n",
    "# 3. PREDYKCJA POCZĄTKOWA\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]  # z poprzedniego kroku\n",
    "\n",
    "# 4. OPTYMALIZACJA PROGU DECYZYJNEGO\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_xgb)\n",
    "\n",
    "TARGET_RECALL = 0.85  # cel\n",
    "optimal_idx = np.argmin(np.abs(recalls - TARGET_RECALL))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Cel: Recall ~ {TARGET_RECALL * 100}%\")\n",
    "print(f\"Optymalny próg: {optimal_threshold:.4f}\")\n",
    "\n",
    "# 5. PREDYKCJA Z NOWYM PROGIEM\n",
    "y_pred_opt = (y_proba_xgb >= optimal_threshold).astype(int)\n",
    "\n",
    "# 6. EWALUACJA FINALNA\n",
    "final_roc_auc = roc_auc_score(y_test, y_proba_xgb)\n",
    "final_recall = recall_score(y_test, y_pred_opt)\n",
    "final_f1_score = f1_score(y_test, y_pred_opt)\n",
    "\n",
    "print(\"WYNIKI\")\n",
    "print(f\"ROC AUC: {final_roc_auc:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1 Score: {final_f1_score:.4f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(classification_report(y_test, y_pred_opt))\n",
    "\n",
    "# 7. WIZUALIZACJA MACIERZY POMYŁEK\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "cmap_opt = sns.light_palette(COLOR_BANKRUPT, as_cmap=True)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_opt)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=cmap_opt,\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Zdrowa\", \"Bankrut\"],\n",
    "    yticklabels=[\"Zdrowa\", \"Bankrut\"],\n",
    ")\n",
    "\n",
    "plt.title(f\"Macierz pomyłek (próg {optimal_threshold:.4f})\")\n",
    "plt.xlabel(\"Przewidziana klasa\")\n",
    "plt.ylabel(\"Prawdziwa klasa\")\n",
    "plt.show()\n",
    "\n",
    "# 8. LOGOWANIE WYNIKÓW DO WANDB\n",
    "wandb.log(\n",
    "    {\n",
    "        \"optimal_threshold\": optimal_threshold,\n",
    "        \"final_auc\": final_roc_auc,\n",
    "        \"final_recall\": final_recall,\n",
    "        \"final_f1\": final_f1_score,\n",
    "        \"final_confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=y_test.values,\n",
    "            preds=y_pred_opt,\n",
    "            class_names=[\"Zdrowa\", \"Bankrut\"],\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
